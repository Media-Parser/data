from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import json
import os
import time
import re
from datetime import datetime

# ë³¸ë¬¸ ì •ì œ í•¨ìˆ˜
def clean_content(soup):
    content_div = soup.select_one("div#bo_v_con")
    if not content_div:
        return ""
    
    result = []
    for p in content_div.find_all("p"):
        txt = p.get_text(strip=True)
        if not txt:
            continue
        # ì¢…ë£Œ ì¡°ê±´: ë‚ ì§œ or ì„œëª…
        if re.search(r"\d{4}\.\s*\d{1,2}\.\s*\d{1,2}\.", txt) or any(word in txt for word in ["ëŒ€ë³€ì¸", "ìœ„ì›ì¥", "ë¶€ëŒ€ë³€ì¸"]):
            break
        result.append(txt)

    return " ".join(result)

# ëŒ€ë³€ì¸, ì œëª© íŒŒì‹±
def parse_meta_title(text):
    if not text:
        return "", ""
    
    text = re.sub(r"\d{4}ë…„\s*\d{1,2}ì›”\s*\d{1,2}ì¼", "", text).strip()
    text = re.sub(r"[|ã…£ï½œ]", "ï½œ", text)
    parts = [p.strip() for p in text.split("ï½œ")]

    if len(parts) >= 3:
        return parts[0], parts[1]
    elif len(parts) == 2:
        return "", parts[0]
    else:
        return "", text

# í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_reformparty():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    driver = webdriver.Chrome(options=chrome_options)

    try:
        save_path = os.path.abspath("bulk_crawling/new_crawlers/data/reform_all.jsonl")
        os.makedirs(os.path.dirname(save_path), exist_ok=True)

        with open(save_path, "a", encoding="utf-8") as f:
            for page in range(1, 97):  # 1 ~ 96
                driver.get(f"https://www.reformparty.kr/briefing?page={page}")
                time.sleep(1.5)

                soup = BeautifulSoup(driver.page_source, "html.parser")
                rows = soup.select("div.tbl_wrap tbody tr")

                for idx, row in enumerate(rows):
                    try:
                        a_tag = row.select_one("div.bo_tit a")
                        date_tag = row.select_one("td.td_datetime")
                        
                        if not a_tag or not date_tag:
                            raise ValueError("ë§í¬ ë˜ëŠ” ë‚ ì§œ íƒœê·¸ ì—†ìŒ")

                        href = a_tag["href"]
                        url = href if href.startswith("http") else f"https://www.reformparty.kr{href}"
                        date_text = date_tag.get_text(strip=True)

                        print(f"[DEBUG] row {idx+1} ë‚ ì§œ ë¬¸ìì—´: '{date_text}'")
                        post_date = datetime.strptime(date_text, "%Y-%m-%d %H:%M:%S").strftime("%Y-%m-%d")

                        # ìƒì„¸ í˜ì´ì§€ ì´ë™
                        driver.get(url)
                        time.sleep(1.5)
                        detail_soup = BeautifulSoup(driver.page_source, "html.parser")

                        raw_title = detail_soup.select_one("span.bo_v_tit").get_text(strip=True)
                        spokesperson, title = parse_meta_title(raw_title)
                        content = clean_content(detail_soup)

                        data = {
                            "ì •ë‹¹": "ê°œí˜ì‹ ë‹¹",
                            "ì œëª©": title,
                            "ëŒ€ë³€ì¸": spokesperson,
                            "ë‚ ì§œ": post_date,
                            "ë³¸ë¬¸": content,
                            "ë§í¬": url
                        }

                        f.write(json.dumps(data, ensure_ascii=False) + "\n")
                        print(f"âœ… ì €ì¥ ì™„ë£Œ: {title[:30]}...")

                        driver.back()
                        time.sleep(1.0)

                    except Exception as e:
                        raise RuntimeError(f"â›” ì˜¤ë¥˜ ë°œìƒ (row {idx+1}): {e}")

        print(f"\nğŸ‰ ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ. ì €ì¥ ìœ„ì¹˜: {save_path}")

    finally:
        driver.quit()

if __name__ == "__main__":
    crawl_reformparty()
